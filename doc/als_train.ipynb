{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('../lib')\n",
    "from SVD import *\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "als = ALS('../data/ratings.csv', True)\n",
    "train_data, test_data = als.split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "als = ALS('../data/ratings.csv')\n",
    "train_data, test_data = als.split(test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_best_params = (5, 0.1, 10, 3)\n",
    "pickle.dump(als_best_params, open( \"../output/als_best_params.p\", \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "als.fit(train_data, num_epoch = 10, elapse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.err(testdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.1, 10, 3),\n",
       " (5, 0.15, 10, 3),\n",
       " (5, 0.2, 10, 3),\n",
       " (5, 0.25, 10, 3),\n",
       " (10, 0.1, 10, 3),\n",
       " (10, 0.15, 10, 3),\n",
       " (10, 0.2, 10, 3),\n",
       " (10, 0.25, 10, 3),\n",
       " (20, 0.1, 10, 3),\n",
       " (20, 0.15, 10, 3),\n",
       " (20, 0.2, 10, 3),\n",
       " (20, 0.25, 10, 3)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_list = [0.1,0.15,0.2,0.25]\n",
    "rank_list = [5,10,20]\n",
    "als.gridParams(reg = reg_list, rank = rank_list)\n",
    "als.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage (5, 0.1, 10, 3)\n",
      "Train rmse : 0.15730247986260262   Cross-Validation rmse : 3.13608492415749\n",
      "Train rmse : 0.15646546385532925   Cross-Validation rmse : 3.510000317863808\n",
      "Train rmse : 0.16858554188334882   Cross-Validation rmse : 4.23802656147735\n",
      "Min cv err: 3.13608492415749\n",
      "stage (10, 0.1, 10, 3)\n",
      "Train rmse : 0.15873798360837146   Cross-Validation rmse : 3.467502124636618\n",
      "Train rmse : 0.1574959647994745   Cross-Validation rmse : 3.9975750482533194\n",
      "Train rmse : 0.16605785883961194   Cross-Validation rmse : 4.582073287307212\n",
      "Min cv err: 3.467502124636618\n",
      "stage (20, 0.1, 10, 3)\n",
      "Train rmse : 0.14981772248940667   Cross-Validation rmse : 3.503392560626259\n",
      "Train rmse : 0.1463924839963924   Cross-Validation rmse : 3.5078025708554295\n",
      "Train rmse : 0.15160390189404838   Cross-Validation rmse : 4.622733283608135\n",
      "Min cv err: 3.503392560626259\n"
     ]
    }
   ],
   "source": [
    "als.tuningParams(test_data, measure = 'rmse', verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 0.1, 10, 3)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rmse : 0.4639893316929484   Cross-Validation rmse : 2.7441895075188447\n",
      "Train rmse : 0.4627235342710854   Cross-Validation rmse : 2.719313973879377\n",
      "Train rmse : 0.4617953103011003   Cross-Validation rmse : 2.7915646847809734\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.719313973879377"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als.cv(reg = s[1], rank = s[0], num_epoch = 10, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rank = 5  : 1.4266937137186813\n",
    "rank = 10 : 1.908464083374105\n",
    "rank = 20 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func.best_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import time\n",
    "\n",
    "\n",
    "class SGD:\n",
    "    \"\"\"\n",
    "    The class for performing collaborative filtering with Stochastic Gradient Descent\n",
    "\n",
    "    Methods for post-processing:\n",
    "        1. KNN\n",
    "\n",
    "    Mesures for evaluation:\n",
    "        1. RMSE: root mean square error\n",
    "        2. MAE : mean absolute error\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,data_dir, sample = False):\n",
    "        \n",
    "        \"\"\"\n",
    "        Attributes:\n",
    "            mean: global mean for rating\n",
    "            bu  : bias associates with users' rating \n",
    "            bi  : bias associates with items' rating\n",
    "            p   : user-matrix\n",
    "            q   : item-matrix\n",
    "        \"\"\"\n",
    "\n",
    "        self.data = pd.read_csv(data_dir)\n",
    "        del self.data['timestamp']\n",
    "        if sample:\n",
    "            self.data = self.data[0:1000]\n",
    "        \n",
    "        self.mean = np.mean(self.data['rating'])\n",
    "        self.user_count = len(self.data['userId'].unique())\n",
    "        self.item_count = len(self.data['movieId'].unique())\n",
    "\n",
    "        self.bu = None\n",
    "        self.bi = None\n",
    "        self.p = None\n",
    "        self.q = None\n",
    "        self.item_dict = None\n",
    "\n",
    "    def split(self,test_size, seed = 0):\n",
    "        train_data, test_data = train_test_split(self.data, test_size = test_size, random_state = seed)\n",
    "        return svdData(train_data), svdData(test_data)\n",
    "    \n",
    "    def fit(self, data, lr = 0.005,reg = 0.4, rank = 10, num_epoch = 10, seed = 0 , stopping_driv = 0.001, elapse = False):\n",
    "        \"\"\"\n",
    "        A method to perform matrix factorization using Stochastic Gradient Descent\n",
    "\n",
    "        lr        : learning rate, Defalt: 10\n",
    "        reg       : regularization parameter: lambda, Defalt: 0.4\n",
    "        rank      : number of latent variables, Default : 10\n",
    "        num_epoch : number of iteration of the SGD procedure, Default:10\n",
    "        seed      : seed of calling random state\n",
    "        \"\"\"\n",
    "        \n",
    "        tmp1 = self.data['movieId'].unique()\n",
    "        self.item_dict = dict(zip(tmp1,[i for i in range(self.item_count)]))\n",
    "            \n",
    "        # initialize user matrix and item matrix\n",
    "        np.random.seed(seed)\n",
    "        p = np.random.normal(2.5,1, size = (self.user_count, rank))\n",
    "        q = np.random.normal(2.5,1, size = (self.item_count, rank))\n",
    "\n",
    "        # initialize bias\n",
    "        bu = np.zeros(self.user_count)\n",
    "        bi = np.zeros(self.item_count)\n",
    "\n",
    "        start_time = time.time()\n",
    "        for this_epoch in range(num_epoch):\n",
    "            \n",
    "            for uid, mid, r in data.ratings:\n",
    "                u = uid - 1\n",
    "                i = self.item_dict[mid]\n",
    "\n",
    "                # prediction\n",
    "                pred = self.mean + bu[u] + bi[i] + np.dot(p[u,:], q[i,:])          \n",
    "                err = r - pred\n",
    "                    \n",
    "                # update bias\n",
    "                deriv =(err - reg * bu[u])\n",
    "                if(np.abs(deriv) > stopping_driv):\n",
    "                    bu[u] += lr * deriv\n",
    "                deriv = (err - reg * bi[i])\n",
    "                if(np.abs(deriv) > stopping_driv):\n",
    "                    bi[i] += lr * deriv\n",
    "\n",
    "                for f in range(rank):\n",
    "                    puf = p[u,f]\n",
    "                    qif = q[i,f]\n",
    "                    deriv = (err * qif - reg * puf)\n",
    "                    if(np.abs(deriv) > stopping_driv):\n",
    "                        p[u,f]  += lr * deriv\n",
    "                    deriv = (err * puf - reg * qif)\n",
    "                    if(np.abs(deriv) > stopping_driv):\n",
    "                        q[i,f]  += lr * deriv\n",
    "        end_time = time.time()\n",
    "        last = round((end_time - start_time), 4)\n",
    "        if elapse:\n",
    "            print(\"Total time: {}s\".format(last))\n",
    "                    \n",
    "        # update the instance variariables\n",
    "        self.bu = bu\n",
    "        self.bi = bi\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        \n",
    "    def err(self, data, measure = 'rmse'):\n",
    "        \"\"\"A method to calculate loss\"\"\"\n",
    "\n",
    "        err = 0\n",
    "\n",
    "        for uid, mid, r in data.ratings:\n",
    "            u = uid - 1\n",
    "            i = self.item_dict[mid]\n",
    "            \n",
    "            if measure == 'rmse':\n",
    "                # prediction\n",
    "                err += (r- self.mean - self.bu[u] - self.bi[i]- np.dot(self.p[u,:], self.q[i,:]))**2\n",
    "                return np.sqrt(err/len(data.ratings))\n",
    "            if measure == 'mae':\n",
    "                err += np.abs(r - self.mean - self.bu[u] - self.bi[i]- np.dot(self.p[u,:], self.q[i,:]))\n",
    "                return err/len(data.ratings)\n",
    "\n",
    "    def KNN(self, testset, K = 5, measure = 'rmse'):\n",
    "        sim = pairwise_distances(self.p, metric = \"cosine\")\n",
    "        err = 0\n",
    "        for idx, row in testset.iterrows():\n",
    "            u = int(row['userId']) - 1\n",
    "            mid = int(row['movieId'])\n",
    "            i = self.item_dict[mid]\n",
    "            neighbor = np.argsort(sim[u])[-K-1:-1]\n",
    "            temp = 0\n",
    "            \n",
    "            for k in range(K):\n",
    "                t = np.dot(self.p[neighbor[k],:], self.q[i,:])\n",
    "                if t != 0:\n",
    "                    temp += t\n",
    "\n",
    "\n",
    "            est_rating = temp/K\n",
    "            if measure == 'rmse':\n",
    "                err += (row['rating'] - est_rating) ** 2\n",
    "                return np.sqrt(err/len(testset))\n",
    "\n",
    "\n",
    "    def kfold_split(self, data, K = 3,seed = 0):\n",
    "        kf = KFold(n_splits= K, random_state = seed, shuffle = True)\n",
    "        trainsets = []\n",
    "        testsets = []\n",
    "        self._K = K\n",
    "        \n",
    "        for train_index, cv_index in kf.split(data.raw):\n",
    "            trainset = data.raw.iloc[train_index,:]\n",
    "            testset = data.raw.iloc[cv_index,:]\n",
    "            \n",
    "            trainsets.append(svdData(trainset))\n",
    "            testsets.append(svdData(testset))\n",
    "        self._trainsets = trainsets\n",
    "        self._testsets = testsets\n",
    "        \n",
    "    def cv(self, data, measure,lr = 0.005, reg = 0.4, rank = 10, num_epoch = 10, verbose = True, plot = False, seed = 0):\n",
    "        \"\"\"A method fo perform cross-validation\"\"\"\n",
    "\n",
    "        train_err= [0] * self._K\n",
    "        cv_err = [0] * self._K\n",
    "\n",
    "        for k in range(self._K):\n",
    "            self.fit(self._trainsets[k], lr = lr,reg = reg,rank = rank,num_epoch = num_epoch)\n",
    "            train_err[k] = self.err(self._trainsets[k], measure)\n",
    "            cv_err[k] = self.err(self._testsets[k], measure)\n",
    "            if verbose:\n",
    "                print(\"Train {} : {}   Cross-Validation {} : {}\".format(measure, round(train_err[k],4), measure, round(cv_err[k],4)))\n",
    "\n",
    "        if plot:\n",
    "            x = np.arange(K) + 1\n",
    "            plt.title(\"Train error and cross-validation error\")\n",
    "            plt.plot(x,train_err, label = \"train error\") \n",
    "            plt.plot(x,cv_err, label = \"cv error\") \n",
    "            plt.xlabel(\"number of folds\")\n",
    "            plt.ylabel(\"{}\".format(measure))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        return np.amin(cv_err)\n",
    "    \n",
    "    def gridParams(self, lr = [0.005], reg = [0.4] ,rank = [10], num_epoch = [10]):\n",
    "        params = []\n",
    "        for l, r, rk, nb in itertools.product(lr, reg, rank, num_epoch):\n",
    "            params.append((l, r, rk, nb))\n",
    "        self.params = params\n",
    "\n",
    "    def tuningParams(self,data, K = 3, measure = 'rmse',verbose = False):\n",
    "        loss = []\n",
    "        self.kfold_split(data, K= K)\n",
    "        for comb in self.params: \n",
    "            err = self.cv(data, measure = measure, lr = comb[0], reg = comb[1], rank = comb[2], num_epoch = comb[3], verbose = verbose)\n",
    "            loss.append(err)\n",
    "            if verbose:\n",
    "                print(\"stage {}\".format(comb))\n",
    "                print(\"Min cv err: {}\".format(err))\n",
    "\n",
    "        idx = np.argmin(loss)\n",
    "        self.best_score = loss[idx]\n",
    "        self.best_params = self.params[idx]\n",
    "\n",
    "\n",
    "\n",
    "class svdData:\n",
    "\n",
    "    \"\"\"\n",
    "    The class for changing data to a format fit ALS algorithm\n",
    "    \"\"\"\n",
    "    def __init__(self, data, R0):\n",
    "\n",
    "    # Q : the user-item matrix based on given data\n",
    "    # R : 1 means the data contains user_u to item_i rating, while 0 means doesn't contain\n",
    "        self.data = data\n",
    "        self.R0 = R0\n",
    "        self.Q, self.R = self._prepare_QR(self.data)\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.raw = data\n",
    "        self.ratings = [(uid, iid, float(r)) for (uid, iid, r) in data.itertuples(index=False)]\n",
    "        \n",
    "\n",
    "    def _prepare_QR(self, data):\n",
    "        temp = self.R0.copy()\n",
    "        for idx, row in data.iterrows():\n",
    "            u = int(row['userId'])\n",
    "            i = int(row['movieId'])\n",
    "            temp[i][u] = row['rating']\n",
    "\n",
    "        Q = temp.values\n",
    "        R = (Q > 0) *1\n",
    "        return Q, R\n",
    "\n",
    "\n",
    "\n",
    "class ALS:\n",
    "    \"\"\"\n",
    "    The class for performing collaborative filtering with Alternating Least Squares\n",
    "\n",
    "    Mesures for evaluation:\n",
    "        1. RMSE: root mean square error\n",
    "        2. MAE : mean absolute error\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, sample = False):\n",
    "        self.df = pd.read_csv(data_dir)\n",
    "        del self.df['timestamp']\n",
    "        if sample:\n",
    "            self.df = self.df[0:1000]\n",
    "        self.data = (self.df.pivot(index='userId', columns='movieId', values='rating')).fillna(0)\n",
    "        # user-item matrix\n",
    "        self.Q = self.data.values\n",
    "        self.R0 = self.data * 0\n",
    "        # user matrix\n",
    "        self.p = None\n",
    "        # item matrix\n",
    "        self.q = None\n",
    "        # kfolds\n",
    "        self.K = None\n",
    "        self.error = None\n",
    "        self.trainsets = None\n",
    "        self.testsets = None\n",
    "        self.params = None\n",
    "        self.best_params = None\n",
    "        \n",
    "        \n",
    "                    \n",
    "    def split(self,test_size = 0.25, seed = 0):\n",
    "        \"\"\"A method for train_test_split\"\"\"\n",
    "        train_data, test_data = train_test_split(self.df,test_size = test_size, random_state = seed)\n",
    "        train_data = svdData(train_data, self.R0)\n",
    "        test_data = svdData(test_data, self.R0)\n",
    "        self.trainsets = [train_data]\n",
    "        self.testsets = [test_data]\n",
    "        return train_data , test_data\n",
    "            \n",
    "    \n",
    "    def fit(self, data, rank = 10, reg = 0.1, num_epoch = 10, measure = 'rmse', elapse = False):\n",
    "        \"\"\"\n",
    "        A method to perform matrix factorization\n",
    "\n",
    "        data      : An svdData format\n",
    "        reg       : regularization parameter: lambda, Defalt: 0.4\n",
    "        rank      : number of latent variables, Default : 10\n",
    "        num_epoch : number of iteration of the SGD procedure, Default:10\n",
    "        measure   : evaluation method\n",
    "        elapse    : if true, print the time of fitting \n",
    "\n",
    "        \"\"\"\n",
    "        I = np.eye(rank)\n",
    "        np.random.seed(0)\n",
    "        p = np.random.normal(2.5,1, size = (self.Q.shape[0], rank))\n",
    "        q = np.random.normal(2.5,1, size = (self.Q.shape[1],rank))\n",
    "\n",
    "        start_time = time.time()\n",
    "        for this_epoch in range(num_epoch):\n",
    "\n",
    "            for u, Iu in enumerate(data.R):\n",
    "                j = np.nonzero(Iu)[0]\n",
    "                nu = sum(Iu)\n",
    "                if nu != 0:\n",
    "                    A = q[j,:].T.dot(q[j,:]) + nu * reg * I\n",
    "                    r = data.Q[u][data.Q[u]!=0]\n",
    "                    V = q[j,:].T.dot(r.T)\n",
    "                    p[u,:] = np.dot(np.linalg.inv(A), V)\n",
    "\n",
    "            for i, Ii in enumerate(data.R.T):\n",
    "                j = np.nonzero(Ii)[0]\n",
    "                ni = sum(Ii)\n",
    "                if ni != 0:\n",
    "                    A = p[j,:].T.dot(p[j,:]) + ni * reg * I\n",
    "                    r = data.Q.T[i][data.Q.T[i]!=0]\n",
    "                    V = p[j,:].T.dot(r) \n",
    "                    q[i,:] = np.dot(np.linalg.inv(A), V)\n",
    "        end_time = time.time()\n",
    "        last = round((end_time - start_time), 4)\n",
    "        if elapse:\n",
    "             print(\"Total time: {}s\".format(last))\n",
    "        self.q = q\n",
    "        self.p = p\n",
    "        self.error = self.err(data)\n",
    "        \n",
    "        \n",
    "    def err(self, data, measure = 'rmse'):\n",
    "        \"\"\"data: als data\"\"\"\n",
    "        if measure == 'rmse':\n",
    "            loss = np.sum((data.R * (self.Q - np.dot(self.p, self.q.T))) ** 2)/ np.sum(data.R)\n",
    "            return np.sqrt(loss)\n",
    "        if measure == 'mae':\n",
    "            loss = np.sum(np.abs(data.R * (self.Q - np.dot(self.p, self.q.T))))/ np.sum(data.R) \n",
    "            return loss\n",
    "    \n",
    "    def kfolds_split(self, als_data, K = 3, seed = 0):\n",
    "        kf = KFold(n_splits= K, random_state = seed, shuffle = True)\n",
    "        self.K = K\n",
    "        trainsets = []\n",
    "        cvsets = []\n",
    "        \n",
    "        for train_index, cv_index in kf.split(als_data.data):\n",
    "            trainset = svdData(als_data.data.iloc[train_index,:], self.R0)\n",
    "            cvset = svdData(als_data.data.iloc[cv_index,:], self.R0)\n",
    "            \n",
    "            trainsets.append(trainset)\n",
    "            cvsets.append(cvset)\n",
    "        self.trainsets = trainsets\n",
    "        self.cvsets = cvsets\n",
    "        \n",
    "    def cv(self,rank = 10, reg = 0.1, num_epoch = 10, measure = 'rmse', verbose = True, plot = False):\n",
    "        train_loss = []\n",
    "        test_loss = []\n",
    "        for k in range(self.K):\n",
    "            train = self.trainsets[k]\n",
    "            test = self.cvsets[k]\n",
    "            self.fit(train, rank = rank, num_epoch = num_epoch, measure = measure)\n",
    "            train_loss.append(self.error)\n",
    "            test_loss.append(self.err(test,measure = measure))\n",
    "            \n",
    "            if verbose:\n",
    "                print(\"Train {} : {}   Cross-Validation {} : {}\".format(measure, train_loss[k], measure,test_loss[k]))\n",
    "        \n",
    "        if plot:\n",
    "            x = np.arange(self.K) + 1\n",
    "            plt.title(\"Train error and cross-validation error\")\n",
    "            plt.plot(x,train_loss, label = \"train error\") \n",
    "            plt.plot(x,test_loss, label = \"test error\") \n",
    "            plt.xlabel(\"number of folds\")\n",
    "            plt.ylabel(\"{}\".format(measure))\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "        \n",
    "        return np.min(test_loss)\n",
    "    \n",
    "    def gridParams(self, rank = [10],  reg = [0.1] , num_epoch = [10], K = [3]):\n",
    "        params = []\n",
    "        for rk, r, np, k in itertools.product(rank, reg, num_epoch, K):\n",
    "            params.append((rk, r, np, k))\n",
    "        self.params = params\n",
    "    \n",
    "    def tuningParams(self, data, measure = 'rmse', verbose = True):\n",
    "        self.kfolds_split(data)\n",
    "        loss = []\n",
    "\n",
    "        for comb in self.params:\n",
    "            test_err = self.cv(measure = measure, rank = comb[0], reg = comb[1], num_epoch = comb[2],verbose = verbose)\n",
    "            loss.append(test_err)\n",
    "            if verbose == True:\n",
    "                print(\"stage {}\".format(comb))\n",
    "                print(\"Min cv err: {}\".format(test_err))\n",
    "\n",
    "        idx = np.argmin(loss)\n",
    "        self.best_score = loss[idx]\n",
    "        self.best_params = self.params[idx]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGD('../data/ratings.csv',sample = True)\n",
    "trainset, testset = sgd.split(0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024218890857483147"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.err(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_list = [0.2,0.25]\n",
    "lr_list = [0.001,0.005]\n",
    "rank_list = [5,10]\n",
    "sgd.gridParams(reg = reg_list, lr = lr_list, rank = rank_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train rmse : 0.0587   Cross-Validation rmse : 0.0723\n",
      "Train rmse : 0.0337   Cross-Validation rmse : 0.0966\n",
      "Train rmse : 0.052   Cross-Validation rmse : 0.051\n",
      "stage (0.001, 0.2, 5, 10)\n",
      "Min cv err: 0.050951704714524496\n",
      "Train rmse : 0.0476   Cross-Validation rmse : 0.1149\n",
      "Train rmse : 0.0268   Cross-Validation rmse : 0.0925\n",
      "Train rmse : 0.038   Cross-Validation rmse : 0.0455\n",
      "stage (0.001, 0.2, 10, 10)\n",
      "Min cv err: 0.0455240105111246\n",
      "Train rmse : 0.0591   Cross-Validation rmse : 0.074\n",
      "Train rmse : 0.0352   Cross-Validation rmse : 0.0965\n",
      "Train rmse : 0.0526   Cross-Validation rmse : 0.0503\n",
      "stage (0.001, 0.25, 5, 10)\n",
      "Min cv err: 0.05029689869121016\n",
      "Train rmse : 0.0479   Cross-Validation rmse : 0.1144\n",
      "Train rmse : 0.0276   Cross-Validation rmse : 0.0916\n",
      "Train rmse : 0.0391   Cross-Validation rmse : 0.0434\n",
      "stage (0.001, 0.25, 10, 10)\n",
      "Min cv err: 0.04335079422173299\n",
      "Train rmse : 0.0463   Cross-Validation rmse : 0.1122\n",
      "Train rmse : 0.0368   Cross-Validation rmse : 0.0823\n",
      "Train rmse : 0.0445   Cross-Validation rmse : 0.0423\n",
      "stage (0.005, 0.2, 5, 10)\n",
      "Min cv err: 0.04229528081807736\n",
      "Train rmse : 0.019   Cross-Validation rmse : 0.1279\n",
      "Train rmse : 0.015   Cross-Validation rmse : 0.0601\n",
      "Train rmse : 0.0298   Cross-Validation rmse : 0.0106\n",
      "stage (0.005, 0.2, 10, 10)\n",
      "Min cv err: 0.010563753912472487\n",
      "Train rmse : 0.0469   Cross-Validation rmse : 0.1124\n",
      "Train rmse : 0.0375   Cross-Validation rmse : 0.0819\n",
      "Train rmse : 0.0452   Cross-Validation rmse : 0.0416\n",
      "stage (0.005, 0.25, 5, 10)\n",
      "Min cv err: 0.04158091104956729\n",
      "Train rmse : 0.0205   Cross-Validation rmse : 0.1267\n",
      "Train rmse : 0.016   Cross-Validation rmse : 0.0602\n",
      "Train rmse : 0.0311   Cross-Validation rmse : 0.0061\n",
      "stage (0.005, 0.25, 10, 10)\n",
      "Min cv err: 0.006145800612974249\n"
     ]
    }
   ],
   "source": [
    "sgd.tuningParams(trainset, measure = 'rmse', verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005, 0.25, 10, 10)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
